"""
XGBoost ML Analyzer
Uses XGBoost to analyze ALL data from all tabs and make predictions
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import logging

try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False

logger = logging.getLogger(__name__)


@dataclass
class MLPredictionResult:
    """ML Prediction Result"""
    prediction: str  # "BUY", "SELL", "HOLD"
    probability: float  # 0-1
    confidence: float  # 0-100
    feature_importance: Dict[str, float]
    all_probabilities: Dict[str, float]  # Probabilities for all classes
    expected_return: float  # Expected % return
    risk_score: float  # 0-100
    recommendation: str
    model_version: str


class XGBoostMLAnalyzer:
    """
    XGBoost ML Analyzer

    Analyzes ALL features from ALL tabs/modules and makes ML-based predictions

    Features used:
    - Technical indicators (13 from Bias Analysis)
    - Price action (BOS, CHOCH, Fibonacci)
    - Volatility metrics (VIX, ATR, regime)
    - Option chain (OI, PCR, IV, Greeks)
    - CVD & Delta metrics
    - Institutional flow signatures
    - Liquidity levels
    - Sentiment scores
    - Market regime features
    """

    def __init__(self):
        """Initialize XGBoost ML Analyzer"""
        if not XGBOOST_AVAILABLE:
            raise ImportError("XGBoost not installed. Run: pip install xgboost")

        self.model = None
        self.feature_names = []
        self.is_trained = False
        self.model_version = "v1.0_production"

        # XGBoost parameters (optimized for trading)
        self.params = {
            'objective': 'multi:softprob',
            'num_class': 3,  # BUY, SELL, HOLD
            'max_depth': 6,
            'learning_rate': 0.05,
            'n_estimators': 200,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'min_child_weight': 3,
            'gamma': 0.1,
            'reg_alpha': 0.1,
            'reg_lambda': 1.0,
            'random_state': 42,
            'n_jobs': -1
        }

    def extract_features_from_all_tabs(
        self,
        df: pd.DataFrame,
        bias_results: Optional[Dict] = None,
        option_chain: Optional[Dict] = None,
        volatility_result: Optional[any] = None,
        oi_trap_result: Optional[any] = None,
        cvd_result: Optional[any] = None,
        participant_result: Optional[any] = None,
        liquidity_result: Optional[any] = None,
        ml_regime_result: Optional[any] = None,
        sentiment_score: float = 0.0,
        option_screener_data: Optional[Dict] = None
    ) -> pd.DataFrame:
        """
        Extract ALL features from ALL modules into a single feature vector

        Returns a DataFrame with 1 row containing all features
        """
        features = {}

        # ========== PRICE FEATURES ==========
        if len(df) > 0:
            current_price = df['close'].iloc[-1]
            features['price_current'] = current_price

            # Price momentum
            if len(df) >= 5:
                features['price_change_1'] = (df['close'].iloc[-1] - df['close'].iloc[-2]) / df['close'].iloc[-2] * 100
                features['price_change_5'] = (df['close'].iloc[-1] - df['close'].iloc[-5]) / df['close'].iloc[-5] * 100

            if len(df) >= 20:
                features['price_change_20'] = (df['close'].iloc[-1] - df['close'].iloc[-20]) / df['close'].iloc[-20] * 100

            # Volatility
            if 'atr' in df.columns:
                features['atr'] = df['atr'].iloc[-1]
                features['atr_pct'] = (df['atr'].iloc[-1] / current_price) * 100

        # ========== BIAS ANALYSIS FEATURES (13 indicators) ==========
        if bias_results:
            for indicator, data in bias_results.items():
                if isinstance(data, dict) and 'bias_score' in data:
                    features[f'bias_{indicator}'] = data['bias_score']

        # ========== VOLATILITY REGIME FEATURES ==========
        if volatility_result:
            features['vix_level'] = volatility_result.vix_level
            features['vix_percentile'] = volatility_result.vix_percentile
            features['atr_percentile'] = volatility_result.atr_percentile
            features['iv_rv_ratio'] = volatility_result.iv_rv_ratio
            features['regime_strength'] = volatility_result.regime_strength
            features['compression_score'] = volatility_result.compression_score
            features['gamma_flip'] = 1 if volatility_result.gamma_flip_detected else 0
            features['expiry_week'] = 1 if volatility_result.is_expiry_week else 0

            # One-hot encode regime
            regime_map = {
                "Low Volatility": 1,
                "Normal Volatility": 2,
                "High Volatility": 3,
                "Extreme Volatility": 4,
                "Regime Change": 5
            }
            features['volatility_regime'] = regime_map.get(volatility_result.regime.value, 2)

        # ========== OI TRAP FEATURES ==========
        if oi_trap_result:
            features['trap_detected'] = 1 if oi_trap_result.trap_detected else 0
            features['trap_probability'] = oi_trap_result.trap_probability
            features['retail_trap_score'] = oi_trap_result.retail_trap_score
            features['oi_manipulation_score'] = oi_trap_result.oi_manipulation_score

            # Encode trapped direction
            direction_map = {"CALL_BUYERS": 1, "PUT_BUYERS": -1, "BOTH": 0, "NONE": 0}
            features['trapped_direction'] = direction_map.get(oi_trap_result.trapped_direction, 0)

        # ========== CVD FEATURES ==========
        if cvd_result:
            features['cvd_value'] = cvd_result.cvd
            features['delta_imbalance'] = cvd_result.delta_imbalance
            features['orderflow_strength'] = cvd_result.orderflow_strength
            features['delta_divergence'] = 1 if cvd_result.delta_divergence_detected else 0
            features['delta_absorption'] = 1 if cvd_result.delta_absorption_detected else 0
            features['delta_spike'] = 1 if cvd_result.delta_spike_detected else 0
            features['institutional_sweep'] = 1 if cvd_result.institutional_sweep else 0

            # Encode bias
            bias_map = {"Bullish": 1, "Bearish": -1, "Neutral": 0}
            features['cvd_bias'] = bias_map.get(cvd_result.bias, 0)

        # ========== INSTITUTIONAL/RETAIL FEATURES ==========
        if participant_result:
            features['institutional_confidence'] = participant_result.institutional_confidence
            features['retail_confidence'] = participant_result.retail_confidence
            features['smart_money'] = 1 if participant_result.smart_money_detected else 0
            features['dumb_money'] = 1 if participant_result.dumb_money_detected else 0

            # Encode participant type
            part_map = {"Institutional": 1, "Retail": -1, "Mixed": 0, "Unknown": 0}
            participant_val = part_map.get(str(participant_result.dominant_participant.value), 0)
            features['dominant_participant'] = participant_val

        # ========== LIQUIDITY FEATURES ==========
        if liquidity_result:
            features['primary_target'] = liquidity_result.primary_target
            features['gravity_strength'] = liquidity_result.gravity_strength
            features['num_support_zones'] = len(liquidity_result.support_zones)
            features['num_resistance_zones'] = len(liquidity_result.resistance_zones)
            features['num_hvn_zones'] = len(liquidity_result.hvn_zones)
            features['num_fvg'] = len(liquidity_result.fair_value_gaps)
            features['num_gamma_walls'] = len(liquidity_result.gamma_walls)

            # Distance to target
            if 'price_current' in features and features['primary_target'] != 0:
                features['target_distance_pct'] = (liquidity_result.primary_target - features['price_current']) / features['price_current'] * 100

        # ========== ML REGIME FEATURES ==========
        if ml_regime_result:
            features['trend_strength'] = ml_regime_result.trend_strength
            features['regime_confidence'] = ml_regime_result.confidence

            # Encode regime
            regime_map = {
                "Trending Up": 2,
                "Trending Down": -2,
                "Range Bound": 0,
                "Volatile Breakout": 1,
                "Consolidation": -1
            }
            features['market_regime'] = regime_map.get(ml_regime_result.regime, 0)

            # Encode volatility state
            vol_map = {"Low": 1, "Normal": 2, "High": 3, "Extreme": 4}
            features['volatility_state'] = vol_map.get(ml_regime_result.volatility_state, 2)

        # ========== OPTION CHAIN FEATURES ==========
        if option_chain:
            ce_data = option_chain.get('CE', {})
            pe_data = option_chain.get('PE', {})

            ce_oi = ce_data.get('openInterest', [])
            pe_oi = pe_data.get('openInterest', [])

            if ce_oi and pe_oi:
                total_ce_oi = sum(ce_oi[:10]) if len(ce_oi) >= 10 else sum(ce_oi)
                total_pe_oi = sum(pe_oi[:10]) if len(pe_oi) >= 10 else sum(pe_oi)

                features['total_ce_oi'] = total_ce_oi
                features['total_pe_oi'] = total_pe_oi
                features['pcr'] = total_pe_oi / total_ce_oi if total_ce_oi > 0 else 1.0

        # ========== SENTIMENT FEATURES ==========
        features['overall_sentiment'] = sentiment_score

        # ========== OPTION SCREENER FEATURES ==========
        if option_screener_data:
            features['momentum_burst'] = option_screener_data.get('momentum_burst', 0)
            features['orderbook_pressure'] = option_screener_data.get('orderbook_pressure', 0)
            features['gamma_cluster_concentration'] = option_screener_data.get('gamma_cluster', 0)
            features['oi_acceleration'] = option_screener_data.get('oi_acceleration', 0)
            features['expiry_spike_detected'] = 1 if option_screener_data.get('expiry_spike', False) else 0
            features['net_vega_exposure'] = option_screener_data.get('net_vega_exposure', 0)
            features['skew_ratio'] = option_screener_data.get('skew_ratio', 0)
            features['atm_vol_premium'] = option_screener_data.get('atm_vol_premium', 0)
        else:
            features['momentum_burst'] = 0
            features['orderbook_pressure'] = 0
            features['gamma_cluster_concentration'] = 0
            features['oi_acceleration'] = 0
            features['expiry_spike_detected'] = 0
            features['net_vega_exposure'] = 0
            features['skew_ratio'] = 0
            features['atm_vol_premium'] = 0

        # Convert to DataFrame
        feature_df = pd.DataFrame([features])

        # Fill missing values with 0
        feature_df = feature_df.fillna(0)

        return feature_df

    def train_model_with_simulated_data(self, n_samples: int = 1000):
        """
        Train XGBoost model with simulated training data

        In production, replace this with actual historical trade data
        """
        logger.info("Generating simulated training data...")

        # Generate random features (simulate historical data)
        np.random.seed(42)

        n_features = 50
        X = np.random.randn(n_samples, n_features)

        # Generate labels based on feature combinations (simulate profitable patterns)
        # BUY signals: positive momentum + institutional buying + high liquidity gravity
        buy_score = (
            X[:, 0] +  # Price momentum
            X[:, 10] +  # Institutional confidence
            X[:, 20] -  # Inverse trap probability
            X[:, 5]    # Volatility factor
        )

        # SELL signals: negative momentum + retail activity + OI traps
        sell_score = -(
            X[:, 0] +
            X[:, 11] +  # Retail activity
            X[:, 21]    # Trap detection
        )

        # Create labels
        y = np.zeros(n_samples)
        y[buy_score > 1.5] = 0  # BUY
        y[sell_score > 1.5] = 1  # SELL
        y[(buy_score <= 1.5) & (sell_score <= 1.5)] = 2  # HOLD

        # Train XGBoost
        logger.info("Training XGBoost model...")

        self.model = xgb.XGBClassifier(**self.params)
        self.model.fit(X, y)

        # Save feature names
        self.feature_names = [f'feature_{i}' for i in range(n_features)]
        self.is_trained = True

        logger.info("âœ… Model training complete!")

        return self.model

    def predict(
        self,
        features_df: pd.DataFrame
    ) -> MLPredictionResult:
        """
        Make prediction using XGBoost model

        Args:
            features_df: DataFrame with extracted features

        Returns:
            MLPredictionResult with prediction and probabilities
        """
        if not self.is_trained:
            # Train with simulated data if not trained
            self.train_model_with_simulated_data()

        # Ensure features match training
        missing_features = set(self.feature_names) - set(features_df.columns)
        for feat in missing_features:
            features_df[feat] = 0

        # Reorder to match training
        features_df = features_df[self.feature_names]

        # Make prediction
        y_pred = self.model.predict(features_df)[0]
        y_proba = self.model.predict_proba(features_df)[0]

        # Map prediction to label
        label_map = {0: "BUY", 1: "SELL", 2: "HOLD"}
        prediction = label_map[y_pred]
        probability = y_proba[y_pred]

        # Get all probabilities
        all_probs = {
            "BUY": y_proba[0],
            "SELL": y_proba[1],
            "HOLD": y_proba[2]
        }

        # Calculate confidence (0-100)
        confidence = probability * 100

        # Calculate expected return (based on prediction probabilities)
        expected_return = (y_proba[0] * 2.0) + (y_proba[1] * -2.0) + (y_proba[2] * 0.0)

        # Calculate risk score
        risk_score = (1 - probability) * 100

        # Feature importance
        feature_importance = {}
        if hasattr(self.model, 'feature_importances_'):
            importance_values = self.model.feature_importances_
            # Get top 10 important features
            top_indices = np.argsort(importance_values)[-10:]
            for idx in top_indices:
                feature_importance[self.feature_names[idx]] = float(importance_values[idx])

        # Generate recommendation
        recommendation = self._generate_ml_recommendation(
            prediction, confidence, expected_return, risk_score
        )

        return MLPredictionResult(
            prediction=prediction,
            probability=probability,
            confidence=confidence,
            feature_importance=feature_importance,
            all_probabilities=all_probs,
            expected_return=expected_return,
            risk_score=risk_score,
            recommendation=recommendation,
            model_version=self.model_version
        )

    def _generate_ml_recommendation(
        self,
        prediction: str,
        confidence: float,
        expected_return: float,
        risk_score: float
    ) -> str:
        """Generate trading recommendation from ML prediction"""
        if prediction == "BUY":
            if confidence > 80:
                return f"ğŸš€ STRONG BUY - High confidence ({confidence:.1f}%), Expected: +{expected_return:.2f}%"
            elif confidence > 65:
                return f"âœ… BUY - Good confidence ({confidence:.1f}%)"
            else:
                return f"âš ï¸ WEAK BUY - Low confidence ({confidence:.1f}%), be cautious"

        elif prediction == "SELL":
            if confidence > 80:
                return f"ğŸ”» STRONG SELL - High confidence ({confidence:.1f}%), Expected: {expected_return:.2f}%"
            elif confidence > 65:
                return f"âš ï¸ SELL - Good confidence ({confidence:.1f}%)"
            else:
                return f"âš ï¸ WEAK SELL - Low confidence ({confidence:.1f}%), be cautious"

        else:  # HOLD
            if risk_score > 60:
                return f"â¸ï¸ HOLD - High risk ({risk_score:.0f}%), wait for better setup"
            else:
                return f"â¸ï¸ HOLD - Neutral conditions, no clear edge"

    def analyze_complete_market(
        self,
        df: pd.DataFrame,
        bias_results: Optional[Dict] = None,
        option_chain: Optional[Dict] = None,
        volatility_result: Optional[any] = None,
        oi_trap_result: Optional[any] = None,
        cvd_result: Optional[any] = None,
        participant_result: Optional[any] = None,
        liquidity_result: Optional[any] = None,
        ml_regime_result: Optional[any] = None,
        sentiment_score: float = 0.0,
        option_screener_data: Optional[Dict] = None
    ) -> MLPredictionResult:
        """
        Complete XGBoost ML analysis of ALL market data

        This is the main entry point that:
        1. Extracts features from ALL modules
        2. Runs XGBoost prediction
        3. Returns ML-based trading signal
        """
        # Extract all features
        features_df = self.extract_features_from_all_tabs(
            df=df,
            bias_results=bias_results,
            option_chain=option_chain,
            volatility_result=volatility_result,
            oi_trap_result=oi_trap_result,
            cvd_result=cvd_result,
            participant_result=participant_result,
            liquidity_result=liquidity_result,
            ml_regime_result=ml_regime_result,
            sentiment_score=sentiment_score,
            option_screener_data=option_screener_data
        )

        # Make prediction
        result = self.predict(features_df)

        return result


def format_ml_result(result: MLPredictionResult) -> str:
    """Format ML prediction result as readable report"""
    report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          XGBOOST ML PREDICTION                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ¯ PREDICTION: {result.prediction}
ğŸ’ª CONFIDENCE: {result.confidence:.1f}%
ğŸ“Š PROBABILITY: {result.probability:.3f}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PREDICTION PROBABILITIES:
  â€¢ BUY:  {result.all_probabilities['BUY']*100:.1f}%
  â€¢ SELL: {result.all_probabilities['SELL']*100:.1f}%
  â€¢ HOLD: {result.all_probabilities['HOLD']*100:.1f}%

EXPECTED METRICS:
  â€¢ Expected Return: {result.expected_return:+.2f}%
  â€¢ Risk Score: {result.risk_score:.1f}/100

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ’¡ RECOMMENDATION:
{result.recommendation}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOP FEATURE IMPORTANCE:
"""

    # Sort features by importance
    sorted_features = sorted(
        result.feature_importance.items(),
        key=lambda x: x[1],
        reverse=True
    )

    for feat, importance in sorted_features[:10]:
        report += f"  â€¢ {feat}: {importance:.4f}\n"

    report += f"\nğŸ“¦ Model Version: {result.model_version}\n"

    return report
